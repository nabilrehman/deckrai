

import { GoogleGenAI, Modality } from "@google/genai";
import { DeckAiExecutionPlan, StyleLibraryItem, Slide, DebugLog, DebugSession, CompanyTheme, PersonalizationAction, TextReplacementAction, ImageReplacementAction, DeckAiTask, EditSlideTask, AddSlideTask } from '../types';

const ai = new GoogleGenAI({ apiKey: import.meta.env.VITE_GEMINI_API_KEY });

export interface BoundingBox {
  x: number;
  y: number;
  width: number;
  height: number;
}

export interface TextReplacement {
    originalText: string;
    newText: string;
}


const fileToGenerativePart = (base64Data: string) => {
    const match = base64Data.match(/^data:(image\/\w+);base64,(.*)$/);
    if (!match) {
        throw new Error("Invalid base64 image data string.");
    }
    const mimeType = match[1];
    const data = match[2];

    return {
        inlineData: {
            data,
            mimeType,
        },
    };
};

const verifyImage = async (originalImage: any, generatedImage: any, prompt: string, logs?: DebugLog[], maskImage?: any): Promise<string | null> => {
    const systemPrompt = `You are a meticulous "Quality Assurance Inspector" AI. Your task is to check a newly generated image for errors by comparing it to the original image and the prompt that was used to create it.

**Your Goal:** Identify any flaws in the generated image, specifically focusing on text accuracy and style consistency.

**Your Process:**
1.  **Analyze the original image** to understand the base style, fonts, and content.
2.  **Analyze the generation prompt** to understand what changes were requested.
3.  **Critically inspect the generated image** for any of the following errors:
    *   **Text Errors:** Are there any typos, misspellings, or garbled text? (e.g., "PostPEGGUL" instead of "PostgreSQL").
    *   **Style Inconsistencies:** Does the new text's font, color, or size fail to match the original slide's style?
    *   **Failed Instructions:** Did the AI fail to follow a specific instruction from the prompt?
4.  **Your Output:**
    *   If you find **ANY** errors, provide a concise, actionable list of corrections for the artist AI. For example: "The text 'PostPEGGUL' is misspelled; it should be 'PostgreSQL'."
    *   If the generated image is a perfect execution of the prompt, respond with the single word: "OK".`;
    
    logs?.push({ title: "QA Inspector: Starting Verification", content: "Comparing generated image against original and prompt." });

    const verificationParts = [
        { text: "Original Image:" },
        originalImage,
        { text: "Generated Image to Verify:" },
        generatedImage,
        { text: `Instructions it was supposed to follow:\n${prompt}` }
    ];

    if (maskImage) {
        verificationParts.push({ text: "Mask (area of change):" });
        verificationParts.push(maskImage);
    }
    verificationParts.push({ text: systemPrompt });


    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: { parts: verificationParts},
    });

    const verificationResult = response.text.trim();
    logs?.push({ title: "QA Inspector: Feedback", content: verificationResult });

    if (verificationResult.toUpperCase() === 'OK') {
        return null; // No corrections needed
    }
    return verificationResult; // Return the correction prompt
};


const generateSingleImage = async (
    model: string,
    imageParts: any[], // Now accepts an array of image parts (original, reference, mask)
    prompt: string,
    deepMode: boolean,
    logs?: DebugLog[],
    onProgress?: (message: string) => void,
): Promise<{ image: string, finalPrompt: string }> => {
    onProgress?.("AI is generating variation...");
    logs?.push({ title: `Artist (${model}): Initial Generation`, content: prompt });

    // --- IMAGEN 4 WORKFLOW ---
    if (model === 'imagen-4.0-generate-001') {
        const response = await ai.models.generateImages({
            model: 'imagen-4.0-generate-001',
            prompt: prompt,
            config: {
                numberOfImages: 1,
                outputMimeType: 'image/png',
                aspectRatio: '16:9',
            },
        });
        const base64ImageBytes = response.generatedImages?.[0]?.image?.imageBytes;
        if (!base64ImageBytes) {
            // FIX: The 'error' property does not exist on the GenerateImagesResponse type.
            // The response for a successful call that generates no image (e.g., due to safety filters)
            // does not include a specific reason in this property.
             throw new Error(`No image was generated by Imagen. (Reason: NO_IMAGE)`);
        }
        return { image: `data:image/png;base64,${base64ImageBytes}`, finalPrompt: prompt };
    }


    // --- GEMINI FLASH WORKFLOW ---
    const parts = [...imageParts, { text: prompt }];
    const config = { responseModalities: [Modality.IMAGE] };

    let response = await ai.models.generateContent({ model, contents: { parts }, config });
    
    let candidate = response.candidates?.[0];
    if (!candidate || !candidate.content?.parts || candidate.content.parts.length === 0) {
        throw new Error(`No image was generated by the AI. (Reason: ${candidate?.finishReason || 'NO_IMAGE'})`);
    }

    let generatedImagePart = candidate.content.parts.find(p => p.inlineData);
    if (!generatedImagePart) throw new Error("No image data found in the AI's response.");
    
    // Self-Correction Step (only in Deep Mode and for Gemini)
    if (deepMode && imageParts.length > 0) { // Self-correction needs an original image to compare against
        onProgress?.("Verifying AI work for accuracy...");
        const correction = await verifyImage(imageParts[0], generatedImagePart, prompt, logs, imageParts.find(p => p.isMask));

        if (correction) {
            onProgress?.("AI is correcting a mistake...");
            const correctedPrompt = `${prompt}\n\n**CRITICAL CORRECTION FROM QA:** You made a mistake on the first attempt. Please regenerate the image, applying this feedback: "${correction}"`;
            
            logs?.push({ title: "Artist: Applying Correction", content: correctedPrompt });
            
            const correctedParts = [...imageParts, { text: correctedPrompt }];
            
            response = await ai.models.generateContent({ model, contents: { parts: correctedParts }, config });
            
            candidate = response.candidates?.[0];
            if (!candidate || !candidate.content?.parts || candidate.content.parts.length === 0) {
                throw new Error(`No image was generated on the correction attempt. (Reason: ${candidate?.finishReason || 'NO_IMAGE'})`);
            }
            
            generatedImagePart = candidate.content.parts.find(p => p.inlineData);
            if (!generatedImagePart) throw new Error("No image data found in the AI's corrected response.");

            const mimeType = generatedImagePart.inlineData!.mimeType;
            const base64ImageBytes = generatedImagePart.inlineData!.data;
            return { image: `data:${mimeType};base64,${base64ImageBytes}`, finalPrompt: correctedPrompt };
        }
    }

    const mimeType = generatedImagePart.inlineData!.mimeType;
    const base64ImageBytes = generatedImagePart.inlineData!.data;
    return { image: `data:${mimeType};base64,${base64ImageBytes}`, finalPrompt: prompt };
};

export const getPersonalizationPlan = async (companyWebsite: string, base64Image: string): Promise<PersonalizationAction[]> => {
    const originalImagePart = fileToGenerativePart(base64Image);

    const systemPrompt = `You are a "Personalization Strategist" AI. Your goal is to create a JSON object outlining specific actions to personalize a slide for a target company, including the location of each change.

**Your Inputs:**
1.  A slide image.
2.  A company website URL: ${companyWebsite}

**Your Task:**
1.  **Analyze the Slide:** Scrutinize the provided slide image to identify its content, key messages, and visual elements (like logos, screenshots, or placeholder text).
2.  **Research the Company:** Use your search tool to analyze the provided company website (${companyWebsite}) to find their branding, products, services, and key terminology.
3.  **Identify Personalization Opportunities:** Find generic content on the slide and formulate precise actions to replace it with specific information from the company's website.
4.  **Determine Bounding Boxes:** For each action, you MUST identify the location and size of the content to be replaced. Provide this as a \`bounding_box\` object with normalized coordinates (from 0.0 to 1.0 for x, y, width, and height relative to the image dimensions).
5.  **Construct the JSON Output:** Your final output MUST be a valid JSON object. Do not add any other explanation or text outside of the JSON object. The object must contain two keys: \`text_replacements\` and \`image_replacements\`.
    *   Each object in \`text_replacements\` must have: \`originalText\`, \`newText\`, and \`bounding_box\`.
    *   Each object in \`image_replacements\` must have: \`areaToReplace\`, \`replacementPrompt\`, and \`bounding_box\`.

**CRITICAL:** If no personalization is possible, return an empty JSON object like \`{"text_replacements": [], "image_replacements": []}\`.`;
    
    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: { parts: [originalImagePart, {text: systemPrompt}] },
        config: { 
          tools: [{ googleSearch: {} }],
        }
    });
    
    const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    
    try {
        const parsedJson = JSON.parse(jsonText);
        const textActions: TextReplacementAction[] = (parsedJson.text_replacements || []).map((a: any) => ({ type: 'TEXT_REPLACEMENT', originalText: a.originalText, newText: a.newText, boundingBox: a.bounding_box }));
        const imageActions: ImageReplacementAction[] = (parsedJson.image_replacements || []).map((a: any) => ({ type: 'IMAGE_REPLACEMENT', areaToReplace: a.areaToReplace, replacementPrompt: a.replacementPrompt, boundingBox: a.bounding_box }));
        return [...textActions, ...imageActions].filter(a => a.boundingBox); // Only return actions that have a valid bounding box
    } catch (e) {
        console.error("Failed to parse personalization plan from AI:", e, jsonText);
        throw new Error("The AI strategist failed to generate a valid personalization plan.");
    }
};

export const getPersonalizedVariationsFromPlan = async (plan: PersonalizationAction[], base64Image: string, deepMode: boolean, onProgress: (message: string) => void): Promise<{ images: string[], logs: DebugLog[], variationPrompts: string[] }> => {
    const logs: DebugLog[] = [{ title: "Workflow Started", content: "Personalize Slide From Plan" }];
    const originalImagePart = fileToGenerativePart(base64Image);
    
    logs.push({ title: "Strategist Plan", content: JSON.stringify(plan, null, 2) });
    
    const textActions = plan.filter(a => a.type === 'TEXT_REPLACEMENT') as TextReplacementAction[];
    const imageActions = plan.filter(a => a.type === 'IMAGE_REPLACEMENT') as ImageReplacementAction[];

    onProgress(`Executing content plan. Found ${textActions.length} text and ${imageActions.length} image replacement(s).`);

    let artistPrompt = `You are a "High-Fidelity Artist". Your task is to edit the provided slide image by performing the following actions:\n`;
    if (textActions.length > 0) artistPrompt += `\n**Text Replacements:**\n${textActions.map(r => `- Replace the text "${r.originalText}" with "${r.newText}"`).join('\n')}\n`;
    if (imageActions.length > 0) artistPrompt += `\n**Image Replacements:**\n${imageActions.map(r => `- In the area described as '${r.areaToReplace}', replace it with '${r.replacementPrompt}'`).join('\n')}\n`;
    artistPrompt += `\n**CRITICAL RULES:**\n1. For text changes, the new text must perfectly match the font, color, size, position, and style of the text it is replacing.\n2. For image changes, the new image should fit seamlessly into the slide's design.\n3. **DO NOT** change any other part of the slide.`;

    const basePrompts = [
        artistPrompt,
        `${artistPrompt}\n(For this version, try a slightly more creative or visually distinct style for the new content if possible, while still matching the slide's overall theme.)`,
        `${artistPrompt}\n(For this version, offer another alternative that is clean and professional.)`,
    ];

    onProgress('Generating 3 variations in parallel...');
    const promises = basePrompts.map(p => 
        generateSingleImage('gemini-2.5-flash-image', [originalImagePart], p, deepMode, logs)
    );
    const settledResults = await Promise.allSettled(promises);

    const images: string[] = [];
    const variationPrompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            variationPrompts.push(result.value.finalPrompt);
        } else {
            console.error("A personalization variation failed:", result.reason);
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All personalization attempts failed.');
    }
    
    return { images, logs, variationPrompts };
};


export const getGenerativeVariations = async (model: string, prompt: string, base64Image: string, deepMode: boolean, onProgress: (message: string) => void): Promise<{ images: string[], logs: DebugLog[], variationPrompts: string[] }> => {
    const logs: DebugLog[] = [{ title: "Workflow Started", content: "Generate with AI" }];
    const originalImagePart = fileToGenerativePart(base64Image);

    // If using Imagen, the prompt is used directly. No "Design Analyst" is needed.
    let refinedPrompt = prompt;
    if (model !== 'imagen-4.0-generate-001') {
        onProgress('Briefing AI Design Analyst with your request...');
        const systemPrompt = `You are a world-class AI agent acting as a "Design Analyst". Your job is to analyze a slide image and a user's request to create a perfect, actionable prompt for a generative image model. Your prompt must describe the **FINAL DESIRED STATE** and include these critical rules:
1.  **"It is absolutely critical to preserve all original branding (logos), text, fonts, colors, and specific numbering with 100% accuracy. The final image must be a perfect match in style to the original, with only the requested change applied."**
2.  **"The final image's dimensions must be in a 16:9 aspect ratio, suitable for a presentation slide."**

**User's Request:** "${prompt}"`;
            
        logs.push({ title: "Agent 1: Design Analyst (Input)", content: systemPrompt });

        onProgress('Analyst is creating a high-fidelity visual plan...');
        const response = await ai.models.generateContent({
            model: "gemini-2.5-flash",
            contents: { parts: [originalImagePart, {text: systemPrompt}] },
        });
        
        const analystOutput = response.text.trim();
        if (!analystOutput) throw new Error("The AI Analyst failed to generate a refined prompt.");
        
        refinedPrompt = analystOutput;
        logs.push({ title: "Agent 1: Design Analyst (Raw Output)", content: refinedPrompt });
        onProgress('Plan received. Briefing AI Artist...');
    }

     const basePrompts = [
        refinedPrompt,
        `${refinedPrompt} (Try a slightly different, creative style.)`,
        `${refinedPrompt} (Offer another alternative version.)`,
    ];

    onProgress('Generating 3 variations in parallel...');
    const promises = basePrompts.map(p =>
        generateSingleImage(model, [originalImagePart], p, deepMode, logs)
    );
    const settledResults = await Promise.allSettled(promises);

    const images: string[] = [];
    const variationPrompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            variationPrompts.push(result.value.finalPrompt);
        } else {
            console.error("A generative variation failed:", result.reason);
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All generation attempts failed.');
    }
    
    return { images, logs, variationPrompts };
};


export const getInpaintingVariations = async (
    prompt: string,
    base64Image: string,
    base64Mask: string,
    deepMode: boolean,
    onProgress: (message: string) => void
): Promise<{ images: string[], logs: DebugLog[], variationPrompts: string[] }> => {
    const logs: DebugLog[] = [{ title: "Workflow Started", content: "Inpainting with AI" }];
    const originalImagePart = fileToGenerativePart(base64Image);
    const maskImagePart = fileToGenerativePart(base64Mask);

    const inpaintingPrompt = `**Task: Inpainting**\nPerform the following instruction ONLY within the masked area of the image. DO NOT change any pixels outside the masked area.\n\n**Instruction:** "${prompt}"`;
    logs.push({ title: "Agent: Inpainting Artist (Input)", content: inpaintingPrompt });
    onProgress('Briefing AI Artist with your inpainting request...');

    const basePrompts = [
        inpaintingPrompt,
        `${inpaintingPrompt} (Try a slightly more creative interpretation.)`,
        `${inpaintingPrompt} (Offer another clean and professional alternative.)`,
    ];

    onProgress('Generating 3 inpainting variations in parallel...');
    const promises = basePrompts.map(p =>
        generateSingleImage(
            'gemini-2.5-flash-image',
            [originalImagePart, maskImagePart],
            p,
            deepMode,
            logs
        )
    );
    const settledResults = await Promise.allSettled(promises);

    const images: string[] = [];
    const variationPrompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            variationPrompts.push(result.value.finalPrompt);
        } else {
            console.error("An inpainting variation failed:", result.reason);
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All inpainting attempts failed.');
    }
    
    return { images, logs, variationPrompts };
};


// ==================================================================
// "ART DIRECTOR" WORKFLOW with STYLE REFERENCE
// ==================================================================
const extractContentAsJson = async (base64Image: string, onProgress: (message: string) => void, logs: DebugLog[]): Promise<any> => {
    onProgress('Analyzing slide structure and content...');
    const originalImagePart = fileToGenerativePart(base64Image);
    const systemPrompt = `You are a "Content Extraction Bot" specializing in presentation slides. Your task is to analyze an image of a slide and extract its content into a structured JSON object.

**Your Goal:** Create a JSON "blueprint" of the slide's content, preserving its structure (titles, lists, tables) but ignoring its visual style (colors, fonts, exact positions).

**JSON Structure Rules:**
1.  The root object should have a \`layout_type\` key (e.g., "title_and_body", "table", "image_with_caption").
2.  Use a \`title\` key for the main slide title.
3.  Use a \`body\` key for main text blocks or bulleted lists. For lists, use an array of strings.
4.  For tables, use a \`table\` key. The value should be an object with \`headers\` (an array of strings) and \`rows\` (an array of arrays of strings).
5.  Extract any other distinct text elements into descriptive keys (e.g., \`footer_text\`, \`presenter_name\`).

**Example for a Table Slide:**
\`\`\`json
{
  "layout_type": "table",
  "title": "All your modeling needs with structured data covered...",
  "table": {
    "headers": ["Use Case", "BQML Model type", "Example use cases"],
    "rows": [
      ["Forecasting", "ARIMA PLUS X-REG (multivariate)", "Forecasting demand for thousands of SKUs simultaneously..."],
      ["Anomaly Detection", "Time series data: Arima plus Xreg...", "Anomaly detection on a time series of sensor data..."]
    ]
  },
  "footer_text": "Google Cloud"
}
\`\`\`

**CRITICAL:** Your output MUST be a single, valid JSON object and nothing else.`;

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash",
        contents: { parts: [originalImagePart, { text: systemPrompt }] },
        config: { responseMimeType: "application/json" },
    });

    const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    
    try {
        const parsedJson = JSON.parse(jsonText);
        logs.push({ title: "Art Director: Extracted Content Blueprint (JSON)", content: JSON.stringify(parsedJson, null, 2) });
        return parsedJson;
    } catch(e: any) {
        logs.push({ title: "Art Director: FAILED to Extract Content", content: `Error parsing JSON: ${e.message}\n\nRaw AI Output:\n${jsonText}` });
        console.error("Failed to parse structured content from slide:", e, jsonText);
        throw new Error("The AI failed to understand the structure of the original slide.");
    }
};

export const findBestStyleReference = async (
    base64TargetImage: string,
    styleLibrary: StyleLibraryItem[],
    logs: DebugLog[]
): Promise<StyleLibraryItem> => {
    logs.push({ title: "Style Scout: Starting Analysis", content: "Analyzing target slide and style library to find the best match." });
    
    const targetImagePart = fileToGenerativePart(base64TargetImage);
    const referenceImageParts = styleLibrary.map(item => ({...fileToGenerativePart(item.src), itemName: item.name }));

    const systemPrompt = `You are a "Style Scout" AI agent, an expert in presentation design and visual analysis. Your task is to find the single best style reference for a target slide from a provided library of options.

**Your Goal:**
Analyze the target slide to understand its fundamental layout and content structure (e.g., "a 4-box horizontal flow diagram", "a title slide with a large background image", "a two-column text layout"). Then, examine each reference slide in the library and select the ONE whose layout is the closest structural and stylistic match.

**Your Process:**
1.  You will be given one "Target Slide".
2.  You will be given a "Reference Library" containing multiple slides with their names.
3.  Compare the Target Slide's structure to each slide in the Reference Library.
4.  Return a single line of text containing ONLY the name of the best matching reference slide. For example: "Page 3". Do not add any other explanation.`;

    const allRefParts = referenceImageParts.flatMap(part => {
        const { itemName, ...imagePart } = part; // Separate name from valid image part
        return [
            { text: `Reference Name: "${itemName}"` },
            imagePart
        ];
    });

    const contents = [
        { role: 'user', parts: [{ text: systemPrompt }] },
        { role: 'user', parts: [{ text: "--- TARGET SLIDE ---" }, targetImagePart] },
        { role: 'user', parts: [{ text: "--- REFERENCE LIBRARY ---" }, ...allRefParts] },
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents,
    });

    const bestMatchName = response.text.trim();
    const bestMatch = styleLibrary.find(item => item.name === bestMatchName);

    if (bestMatch) {
        logs.push({ title: "Style Scout: Match Found", content: `The best matching style reference is: "${bestMatch.name}"` });
        return bestMatch;
    }

    logs.push({ title: "Style Scout: Match Failed", content: `Could not find a library item named "${bestMatchName}". Falling back to the first item.` });
    // Fallback if the model hallucinates a name
    return styleLibrary[0];
};

export const findBestStyleReferenceFromPrompt = async (
    prompt: string,
    styleLibrary: StyleLibraryItem[],
): Promise<StyleLibraryItem | null> => { // Can return null if library is empty
    const logs: DebugLog[] = []; // Internal logs for this specific function
    if (styleLibrary.length === 0) {
        logs.push({ title: "Style Scout (Text-to-Style): Skipped", content: "Style library is empty. No reference will be used." });
        return null;
    }

    logs.push({ title: "Style Scout (Text-to-Style): Starting Analysis", content: "Analyzing user prompt to find the best style match from the library." });

    const referenceImageParts = styleLibrary.map(item => ({...fileToGenerativePart(item.src), itemName: item.name }));

    const systemPrompt = `You are a "Style Scout" AI agent, an expert in presentation design and visual analysis. Your task is to find the single best style reference for a new slide based on a user's text prompt.

**Your Goal:**
Analyze the user's prompt to understand the INTENDED layout and content structure for a NEW slide (e.g., "a 3-column feature comparison", "an agenda slide", "an architecture diagram"). Then, examine each reference slide in the library and select the ONE whose layout is the closest structural match to the user's intent.

**Your Process:**
1. You will be given one "User Prompt".
2. You will be given a "Reference Library" containing multiple slides with their names.
3. Compare the user's intended structure to each slide in the Reference Library.
4. Return a single line of text containing ONLY the name of the best matching reference slide. For example: "Page 3". Do not add any other explanation.`;

    const allRefParts = referenceImageParts.flatMap(part => {
        const { itemName, ...imagePart } = part;
        return [
            { text: `Reference Name: "${itemName}"` },
            imagePart
        ];
    });

    const contents = [
        { role: 'user', parts: [{ text: systemPrompt }] },
        { role: 'user', parts: [{ text: `--- USER PROMPT --- \n"${prompt}"` }] },
        { role: 'user', parts: [{ text: "--- REFERENCE LIBRARY ---" }, ...allRefParts] },
    ];

    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents,
    });
    
    const bestMatchName = response.text.trim();
    const bestMatch = styleLibrary.find(item => item.name === bestMatchName);

    if (bestMatch) {
        logs.push({ title: "Style Scout (Text-to-Style): Match Found", content: `The best matching style reference is: "${bestMatch.name}"` });
        return bestMatch;
    }

    logs.push({ title: "Style Scout (Text-to-Style): Match Failed", content: `Could not find a library item named "${bestMatchName}". Falling back to the first item.` });
    return styleLibrary[0];
};

export const remakeSlideWithStyleReference = async (
    prompt: string,
    base64Image: string,
    styleLibrary: StyleLibraryItem[],
    deepMode: boolean,
    onProgress: (message: string) => void,
): Promise<{ images: string[], logs: DebugLog[], variationPrompts: string[], bestReferenceSrc: string }> => {
    const logs: DebugLog[] = [{ title: "Workflow Started", content: "Remake Slide with Style Reference" }];
    
    const extractedContentJson = await extractContentAsJson(base64Image, onProgress, logs);
    
    if (styleLibrary.length === 0) {
        throw new Error("No style references found in your library. Add some high-quality slides to your library to use this feature.");
    }
    
    onProgress('AI Style Scout is analyzing your library for the best match...');
    const bestReference = await findBestStyleReference(base64Image, styleLibrary, logs);

    if (!bestReference) {
        throw new Error("The AI Style Scout failed to select a reference slide.");
    }

    const referenceImagePart = fileToGenerativePart(bestReference.src);

    const basePromptForArtist = `You are a "High-Fidelity Slide Designer". Your task is to create a new slide by rendering a structured JSON "blueprint" of content in the visual style of a provided reference slide.

**1. Reference Slide (For Style):**
(An image of the reference slide will be provided)

**2. Content Blueprint (JSON):**
\`\`\`json
${JSON.stringify(extractedContentJson, null, 2)}
\`\`\`

**3. User's High-Level Goal:**
"${prompt}"

**CRITICAL STYLE RULES:**
1. The new slide you create MUST perfectly match the visual style, aesthetics, color palette, font choices (typography), and layout principles of the reference slide.
2. **Handling Nested Content:** When a table cell in the JSON contains an array of strings, render them as a bulleted list or on separate lines, matching the style of the original slide. **DO NOT** render the JSON brackets \`[]\` or quotation marks \`""\`.
3. **Negative Constraint:** Do not invent any new text, headers, or data that is not explicitly present in the JSON blueprint. Your only job is to render the provided content in the new style.
4.  **Aspect Ratio:** Render the new slide in a 16:9 aspect ratio, suitable for a presentation.

**Your Task:**
Render the content from the JSON BLUEPRINT according to the user's GOAL, ensuring the final output is in the exact STYLE of the reference slide and adheres to all CRITICAL STYLE RULES.
`;

    const basePrompts = [
        basePromptForArtist,
        `${basePromptForArtist}\n\n(For this version, interpret the layout with a slightly more creative or visually distinct style, while staying true to the reference.)`,
        `${basePromptForArtist}\n\n(For this version, offer another alternative that is clean and professional, using the reference as your guide.)`,
    ];

    onProgress(`Briefing AI artists with 3 design directions based on "${bestReference.name}"...`);
    const promises = basePrompts.map(p => {
        return generateSingleImage(
            'gemini-2.5-flash-image',
            [referenceImagePart], 
            p,
            deepMode,
            logs
        );
    });
    const settledResults = await Promise.allSettled(promises);
    
    const images: string[] = [];
    const variationPrompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            variationPrompts.push(result.value.finalPrompt);
        } else {
            console.error("A remake variation failed:", result.reason);
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All remake attempts failed.');
    }
    
    return { images, logs, variationPrompts, bestReferenceSrc: bestReference.src };
};



// ==================================================================
// "PLAN AND EXECUTE" DECK PERSONALIZATION WORKFLOW
// ==================================================================
export const generateDeckExecutionPlan = async (userPrompt: string, slidesInfo: { id: string, name: string }[]): Promise<DeckAiExecutionPlan> => {
    const systemPrompt = `You are a "Master Presentation Strategist" AI. Your goal is to create a JSON execution plan to modify a slide deck based on a user's high-level request.

**Your Inputs:**
1.  A user's request (e.g., "Customize this deck for dhl.com, add a POC slide...").
2.  A list of all slides in the deck, with their ID and original name.

**Your Task:**
1.  **Analyze the Request:** Deconstruct the user's prompt into specific, actionable tasks. This can include editing existing slides OR adding new slides.
2.  **Map Tasks to Slides:** Use the provided slide list to identify the correct slide for each task.
3.  **Formulate Detailed Prompts:** For each task, create a concise but comprehensive \`detailed_prompt\` for a "Worker" AI.
4.  **Construct the Plan:** Your final output MUST be a valid JSON object. Do not add any other explanation. The object must have two keys: \`thought_process\` and \`tasks\`.
    *   \`thought_process\`: A string summarizing your plan.
    *   \`tasks\`: An array of task objects. Each task object must have a \`type\`.

**Task Object Schemas:**
*   **If \`type\` is \`'EDIT_SLIDE'\`:** The object MUST contain:
    *   \`slideId\`: string (The ID of the slide to edit, from the provided list)
    *   \`detailed_prompt\`: string (Your instructions for the worker AI)
*   **If \`type\` is \`'ADD_SLIDE'\`:** The object MUST contain:
    *   \`newSlideName\`: string (A concise name for the new slide)
    *   \`insertAfterSlideId\`: string (The ID of the slide that should come *before* the new one. Use "START" to add to the beginning.)
    *   \`detailed_prompt\`: string (Your instructions for creating the new slide)

**CRITICAL:** If a slide does not need to be changed, DO NOT include it in the plan.`;
    
    const slideListForPrompt = slidesInfo.map(s => `- ID: "${s.id}", Name: "${s.name}"`).join('\n');
    const fullPrompt = `User Request: "${userPrompt}"\n\nSlide Deck Structure:\n${slideListForPrompt}`;
    
    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: [{ role: 'user', parts: [{ text: systemPrompt }, { text: fullPrompt }] }],
        config: { 
          tools: [{ googleSearch: {} }],
        }
    });

    const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    try {
        const parsedPlan = JSON.parse(jsonText);
        
        // Create a map for quick slide name lookups
        const slideInfoMap = new Map(slidesInfo.map(s => [s.id, s.name]));

        const validatedTasks: DeckAiTask[] = (parsedPlan.tasks || []).map((rawTask: any) => {
            if (rawTask.type === 'EDIT_SLIDE') {
                const slideId = rawTask.slideId || rawTask.slide_id; // Handle both camelCase and snake_case
                if (!slideId) return null; // Invalid task
                
                const editTask: EditSlideTask = {
                    type: 'EDIT_SLIDE',
                    slideId: slideId,
                    slideName: slideInfoMap.get(slideId) || 'Unknown Slide', // Add the slideName
                    detailed_prompt: rawTask.detailed_prompt,
                };
                return editTask;
            }
            if (rawTask.type === 'ADD_SLIDE') {
                // Assuming ADD_SLIDE task is well-formed as per prompt
                return rawTask as AddSlideTask;
            }
            return null; // Invalid task type
        }).filter((task: DeckAiTask | null): task is DeckAiTask => task !== null);

        const finalPlan: DeckAiExecutionPlan = {
            thought_process: parsedPlan.thought_process,
            tasks: validatedTasks,
        };

        return finalPlan;
    } catch (e) {
        console.error("Failed to parse execution plan from AI:", e, jsonText);
        throw new Error("The AI strategist failed to generate a valid execution plan. Please try rephrasing your request.");
    }
};

export const executeSlideTask = async (base64Image: string, detailedPrompt: string, deepMode: boolean): Promise<{ images: string[], prompts: string[] }> => {
    const artistSystemPrompt = `You are a world-class "High-Fidelity Artist" AI. Your task is to edit the provided slide image based on a specific set of instructions.\n\n**CRITICAL RULES:**\n1.  **Preserve Original Content:** You MUST preserve all original branding, text, fonts, colors, and layout that you are not explicitly told to change.\n2.  **Match the Style:** Any new content (text or images) must perfectly match the visual style of the original slide.\n3.  **Follow Instructions Exactly:** Execute the detailed prompt precisely.\n4.  **Aspect Ratio:** Ensure the final slide maintains a standard 16:9 presentation aspect ratio.\n---\n**Detailed Prompt:** "${detailedPrompt}"`;
    const originalImagePart = fileToGenerativePart(base64Image);
    
    const basePrompts = [
        artistSystemPrompt,
        `${artistSystemPrompt}\n(For this version, interpret the request with a slightly more creative or visually distinct style.)`,
        `${artistSystemPrompt}\n(For this version, offer another clean and professional alternative.)`,
    ];

    const promises = basePrompts.map(p => generateSingleImage('gemini-2.5-flash-image', [originalImagePart], p, deepMode));
    const settledResults = await Promise.allSettled(promises);
    
    const images: string[] = [];
    const prompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            prompts.push(result.value.finalPrompt);
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All image generation attempts failed.');
    }
    return { images, prompts };
};

export const createSlideFromPrompt = async (
    referenceSlideImage: string | null, // Now optional
    detailedPrompt: string,
    deepMode: boolean,
    logs: DebugLog[],
    onProgress?: (message: string) => void,
    theme?: CompanyTheme | null,
    logoImage?: string | null
): Promise<{ images: string[], prompts: string[], logs: DebugLog[] }> => {
    onProgress?.('Briefing AI Creative Slide Designer...');
    logs.push({ title: "Designer Input", content: detailedPrompt });
    
    let themeInstructions = '';
    if (theme) {
        themeInstructions = `\n\n**THEME INSTRUCTIONS:**\n- Primary Color: ${theme.primaryColor}\n- Secondary Color: ${theme.secondaryColor}\n- Accent Color: ${theme.accentColor}\n- Font Style: ${theme.fontStyle}\n- Visual Style: ${theme.visualStyle}`;
    }

    let logoInstructions = '';
    const imageParts = [];
    if (referenceSlideImage) {
        imageParts.push({ text: "--- REFERENCE SLIDE (FOR STYLE) ---" });
        imageParts.push(fileToGenerativePart(referenceSlideImage));
    }
     if (logoImage) {
        logoInstructions = `\n\n**LOGO INSTRUCTION:** You have been provided with an additional image which is the company's logo. You MUST incorporate this logo into the slide design in an appropriate and professional manner (e.g., in a corner or alongside the title).`;
        imageParts.push({ text: "--- CUSTOMER LOGO (TO INCLUDE) ---" });
        imageParts.push(fileToGenerativePart(logoImage));
    }


    const designerSystemPrompt = referenceSlideImage
        ? `You are a "Creative Slide Designer" AI. Your task is to create a brand new slide from scratch based on a detailed prompt.\n\n**CRITICAL RULES:**\n1.  **Reference Style:** You are provided with a reference slide image. The new slide you create MUST perfectly match the visual style, aesthetics, color palette, font choices, and general layout principles of this reference slide.\n2.  **Follow Prompt:** Create the content of the new slide based *only* on the detailed prompt.\n3.  **Aspect Ratio:** The new slide you create MUST be in a 16:9 aspect ratio.\n---\n**Detailed Prompt:** "${detailedPrompt}"${themeInstructions}${logoInstructions}`
        : `You are a "Creative Slide Designer" AI. Your task is to create a brand new slide from scratch based on a detailed prompt.\n\n**CRITICAL RULES:**\n1. **Design Style:** Since no reference image is provided, create a clean, professional, and visually appealing design. Use a modern, minimalist aesthetic with good typography and layout principles.\n2.  **Follow Prompt:** Create the content of the new slide based *only* on the detailed prompt.\n3.  **Aspect Ratio:** The new slide you create MUST be in a 16:9 aspect ratio.\n---\n**Detailed Prompt:** "${detailedPrompt}"${themeInstructions}${logoInstructions}`;
    
    
    const basePrompts = [
        designerSystemPrompt,
        `${designerSystemPrompt}\n(For this version, offer a slightly different but equally professional layout.)`,
        `${designerSystemPrompt}\n(For this version, present a third creative option that still adheres to the style.)`,
    ];

    onProgress?.('Generating 3 new slide variations...');
    const promises = basePrompts.map(p => generateSingleImage('gemini-2.5-flash-image', imageParts, p, deepMode, logs, onProgress));
    const settledResults = await Promise.allSettled(promises);
    
    const images: string[] = [];
    const prompts: string[] = [];
    settledResults.forEach(result => {
        if (result.status === 'fulfilled') {
            images.push(result.value.image);
            prompts.push(result.value.finalPrompt);
        } else {
            logs.push({ title: 'Generation Failed', content: result.reason.toString() });
        }
    });

    if (images.length === 0) {
        const firstError = settledResults.find(r => r.status === 'rejected') as PromiseRejectedResult | undefined;
        throw new Error(firstError?.reason?.message || 'All image generation attempts failed.');
    }
    return { images, prompts, logs };
};


export const analyzeDebugSession = async (session: DebugSession): Promise<string> => {
    const systemPrompt = `You are a "Debug Analyst" AI. Your task is to analyze a JSON object representing a failed AI generation session and provide a root cause analysis.

**Your Goal:** Explain clearly and concisely what went wrong.

**Your Process:**
1.  **Review the entire session JSON.** Pay close attention to the \`workflow\`, \`logs\`, and the final \`error\` message.
2.  **Summarize the Goal:** Briefly state what the user was trying to do (e.g., "The user was trying to remake a slide using a style reference.").
3.  **Pinpoint the Failure:** Identify the exact step in the \`logs\` where the process failed. Look for logs with titles like "FAILED..." or examine the final log before the error occurred.
4.  **Provide Root Cause Analysis:** Based on the logs and the error message, explain *why* it failed. Be specific.
    *   *Example 1:* "The process failed during the 'Content Extraction' step. The logs show the AI's output was not valid JSON, which caused a parsing error. This usually means the original slide was too complex or visually ambiguous for the AI to understand its structure."
    *   *Example 2:* "The process failed during the final 'Artist' generation. The error 'NO_IMAGE' indicates the AI refused to generate an image, likely due to a safety policy violation in the prompt or a prompt that was too abstract."
5.  **Suggest a Solution:** Recommend a specific action the user can take. (e.g., "Try rephrasing the prompt to be more direct," or "This slide may be too complex for the 'remake' feature; try using the inpainting tool for a more targeted edit.").

**Your Output:** A clear, well-structured analysis in Markdown format.`;

    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: [{ text: systemPrompt }, { text: `\n\n--- SESSION LOG TO ANALYZE ---\n\`\`\`json\n${JSON.stringify(session, null, 2)}\n\`\`\``}],
    });
    
    return response.text.trim();
};


// ==================================================================
// "CONTENT STRATEGIST" & "PROMPT ENHANCER" WORKFLOWS
// ==================================================================
export const generateOutlineFromNotes = async (rawNotes: string): Promise<string[]> => {
    const systemPrompt = `You are an expert "Presentation Content Strategist" for a leading tech company. Your specialty is transforming raw, unstructured information (like meeting notes, technical documents, or brainstorms) into a clear, concise, and compelling slide deck outline for a customer presentation.

**Your Task:**
1.  **Read and Deeply Understand** the entire context of the provided text.
2.  **Identify the Key Themes** and narrative pillars: What is the customer's problem? What is our proposed solution? What are the key technical details? What are the benefits? What is the call to action?
3.  **Structure a Logical Narrative Flow** suitable for a professional presentation. Start with a title, an agenda, build up the solution, and end with next steps.
4.  **Break Down the Narrative into Individual Slides.** Be selective. Not every detail needs its own slide. Group related concepts.
5.  For each slide, write a **clear and descriptive prompt** that includes a title and a summary of the key content or bullet points that should be on it. This prompt should be detailed enough for another AI to design the slide.

**CRITICAL CONSTRAINT:** The total number of slides in the outline should be reasonable for a standard business presentation and **MUST NOT exceed 30 slides**. Be concise and combine related topics onto single slides where appropriate.

**Your Output:**
Your final output MUST be a JSON array of strings, where each string is the complete, detailed prompt for one slide. Do not add any other explanation or text outside of the JSON array.`;

    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: [{ text: systemPrompt }, { text: `\n\n--- RAW NOTES TO ANALYZE ---\n${rawNotes}` }],
    });

    const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    try {
        const parsedJson = JSON.parse(jsonText);
        if (Array.isArray(parsedJson) && parsedJson.every(item => typeof item === 'string')) {
            // Add a hard-coded safety net to prevent excessively long decks.
            if (parsedJson.length > 30) {
                return parsedJson.slice(0, 30);
            }
            return parsedJson;
        } else {
            throw new Error("AI output was valid JSON but not an array of strings.");
        }
    } catch (e) {
        console.error("Failed to parse presentation plan from AI:", e, jsonText);
        throw new Error("The AI strategist failed to generate a valid presentation plan. Please try again or refine your notes.");
    }
};


export const enhanceOutlinePrompts = async (originalPrompts: string[]): Promise<string[]> => {
    const systemPrompt = `You are a "Presentation Prompt Enhancer" AI. Your task is to take a user's basic, manually-written slide outline and enrich it to create high-quality, "slide-worthy" prompts for a designer AI.

**Your Goal:**
Transform a simple list of slide titles into a set of detailed, descriptive prompts. The new prompts should guide a designer AI to create visually appealing and content-rich slides.

**Your Process:**
1.  **Analyze the User's Outline:** Read the entire list of prompts to understand the overall topic and flow of the presentation.
2.  **Enrich Each Prompt:** For each prompt in the user's outline, expand upon it.
    *   If it's just a title (e.g., "Agenda"), add some placeholder content (e.g., "Agenda slide with 5 key topics: Introduction, Problem, Solution, Demo, Next Steps.").
    *   If it's a concept (e.g., "Our Solution"), add a descriptive title and break the concept down into a few key bullet points.
    *   Maintain the user's original intent, but make the prompt more descriptive and actionable for a designer.
3.  **Maintain Structure:** The number of prompts in your output MUST match the number of prompts in the input.

**Your Output:**
Your final output MUST be a JSON array of strings, with each string being the new, enhanced prompt. Do not add any other explanation or text outside of the JSON array.`;

    const fullPrompt = `--- USER'S OUTLINE ---\n${originalPrompts.join('\n')}`;
    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: [{ text: systemPrompt }, { text: fullPrompt }],
    });
    
     const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    try {
        const parsedJson = JSON.parse(jsonText);
        if (Array.isArray(parsedJson) && parsedJson.length === originalPrompts.length && parsedJson.every(item => typeof item === 'string')) {
            return parsedJson;
        } else {
            console.warn("Prompt Enhancer output was malformed, falling back to original prompts.");
            return originalPrompts; // Fallback to original prompts if AI fails
        }
    } catch (e) {
        console.error("Failed to parse enhanced prompts from AI, falling back:", e, jsonText);
        return originalPrompts; // Fallback to original prompts on error
    }
};



// Dummy functions for older, unused features to prevent compilation errors if they are still referenced somewhere.
export const editImage = async (base64Image: string, prompt: string, deepMode: boolean = false): Promise<string> => {
    const originalImagePart = fileToGenerativePart(base64Image);
    const { image } = await generateSingleImage('gemini-2.5-flash-image', [originalImagePart], prompt, deepMode);
    return image;
};
export const compositeImage = async (baseImage: string, overlayImage: string, prompt: string): Promise<string> => { throw new Error("Composite feature is not supported in this version.") };

export const generateThemeFromWebsite = async (companyWebsite: string): Promise<CompanyTheme> => {
    const systemPrompt = `You are an expert "Brand Analyst" AI. Your task is to analyze the provided company website to extract its core visual branding elements and synthesize them into a structured JSON theme.

**Your Process:**
1.  **Analyze the Website:** Use your search tool to thoroughly browse the provided URL: ${companyWebsite}. Pay close attention to the company's logo, button colors, background colors, heading fonts, and overall aesthetic (e.g., minimalist, corporate, playful).
2.  **Extract Core Elements:** Identify the primary, secondary, and accent colors (as hex codes). Determine the style of typography used (e.g., "a modern, clean sans-serif font"). Describe the overall visual style in a few keywords.
3.  **Return a Structured JSON Object:** Your final output MUST be a valid JSON object. Do not add any other explanation or text outside of the JSON object. The object must have the following keys: \`primaryColor\`, \`secondaryColor\`, \`accentColor\`, \`fontStyle\`, \`visualStyle\`.`;

    const response = await ai.models.generateContent({
        model: "gemini-2.5-pro",
        contents: [{ text: systemPrompt }],
        config: {
            tools: [{ googleSearch: {} }],
        },
    });

    const jsonText = response.text.trim().replace(/^```json\s*|```\s*$/g, '');
    try {
        const parsedJson = JSON.parse(jsonText);
        // Basic validation
        if (parsedJson.primaryColor && parsedJson.fontStyle && parsedJson.visualStyle) {
            return parsedJson as CompanyTheme;
        } else {
            throw new Error("AI output was valid JSON but was missing required theme properties.");
        }
    } catch (e) {
        console.error("Failed to parse theme from AI:", e, jsonText);
        throw new Error("The AI brand analyst failed to generate a valid theme. The website might be inaccessible or too complex.");
    }
};
